
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a researcher at Intelligent Imaging group at Samsung Research America. My research interests are primarily in the overlapping areas of image/video processing, computer vision and computational photography. I also interested in applying self-supervision and generative modeling techniques to various imaging applications. Previously, I did my PhD in the beautiful city of Austin, TX, at UT Austin under the ‘great’ Dr. Alan Bovik’s supervision!\nYou can find my resume here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a researcher at Intelligent Imaging group at Samsung Research America. My research interests are primarily in the overlapping areas of image/video processing, computer vision and computational photography. I also interested in applying self-supervision and generative modeling techniques to various imaging applications.","tags":null,"title":"Pavan Chennagiri","type":"authors"},{"authors":["Qi Zheng","Zhengzhong Tu","Pavan C. Madhusudana","Xiaoyang Zeng","Alan C. Bovik","Yibo Fan"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"77adc5a2c4cc9db670f99e7896afc29b","permalink":"https://pavancm.github.io/publication/faver/","publishdate":"2024-02-04T02:21:33.99755Z","relpermalink":"/publication/faver/","section":"publication","summary":"Video quality assessment (VQA) remains an important and challenging problem that affects many applications at the widest scales. Recent advances in mobile devices and cloud computing techniques have made it possible to capture, process, and share high resolution, high frame rate (HFR) videos across the Internet nearly instantaneously. Being able to monitor and control the quality of these streamed videos can enable the delivery of more enjoyable content and perceptually optimized rate control. Accordingly, there is a pressing need to develop VQA models that can be deployed at enormous scales. While some recent effects have been applied to full-reference (FR) analysis of variable frame rate and HFR video quality, the development of no-reference (NR) VQA algorithms targeting frame rate variations has been little studied. Here, we propose a first-of-a-kind blind VQA model for evaluating HFR videos, which we dub the Framerate-Aware Video Evaluator w/o Reference (FAVER). FAVER uses extended models of spatial natural scene statistics that encompass space–time wavelet-decomposed video signals, and leverages the advantages of the deep neural network to provide motion perception, to conduct efficient frame rate sensitive quality prediction. Our extensive experiments on several HFR video quality datasets show that FAVER outperforms other blind VQA algorithms at a reasonable computational cost. To facilitate reproducible research and public evaluation, an implementation of FAVER is being made freely available online: https://github.com/uniqzheng/HFR-BVQA.","tags":["Video quality assessment","High frame rate","No reference/blind","Temporal band-pass filter","Natural scene statistics","Generalized Gaussian distribution"],"title":"FAVER: Blind quality prediction of variable frame rate videos","type":"publication"},{"authors":["Pavan C. Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"7e75190c9ffa75deb67a08e4d12719ab","permalink":"https://pavancm.github.io/publication/conviqt/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/conviqt/","section":"publication","summary":"Perceptual video quality assessment (VQA) is an integral component of many streaming and video sharing platforms. Here we consider the problem of learning perceptually relevant video quality representations in a self-supervised manner. Distortion type identification and degradation level determination is employed as an auxiliary task to train a deep learning model containing a deep Convolutional Neural Network (CNN) that extracts spatial features, as well as a recurrent unit that captures temporal information. The model is trained using a contrastive loss and we therefore refer to this training framework and resulting model as CONtrastive VIdeo Quality EstimaTor (CONVIQT). During testing, the weights of the trained model are frozen, and a linear regressor maps the learned features to quality scores in a no-reference (NR) setting. We conduct comprehensive evaluations of the proposed model against leading algorithms on multiple VQA databases containing wide ranges of spatial and temporal distortions. We analyze the correlations between model predictions and ground-truth quality ratings, and show that CONVIQT achieves competitive performance when compared to state-of-the-art NR-VQA models, even though it is not trained on those databases. Our ablation experiments demonstrate that the learned representations are highly robust and generalize well across synthetic and realistic distortions. Our results indicate that compelling representations with perceptual bearing can be obtained using self-supervised learning.","tags":["Distortion;Video recording;Quality assessment;Feature extraction;Streaming media;Training;Predictive models;No reference video quality assessment;blind video quality assessment;self-supervised learning;deep learning"],"title":"CONVIQT: Contrastive Video Quality Estimator","type":"publication"},{"authors":["Pavan C. Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1655164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655164800,"objectID":"6a9cd04e7dd26679ce975345fbf0f1a2","permalink":"https://pavancm.github.io/publication/contrique/","publishdate":"2022-06-14T00:00:00Z","relpermalink":"/publication/contrique/","section":"publication","summary":"We consider the problem of obtaining image quality representations in a self-supervised manner. We use prediction of distortion type and degree as an auxiliary task to learn features from an unlabeled image dataset containing a mixture of synthetic and realistic distortions. We then train a deep Convolutional Neural Network (CNN) using a contrastive pairwise objective to solve the auxiliary problem. We refer to the proposed training framework and resulting deep IQA model as the CONTRastive Image QUality Evaluator (CONTRIQUE). During evaluation, the CNN weights are frozen and a linear regressor maps the learned representations to quality scores in a No-Reference (NR) setting. We show through extensive experiments that CONTRIQUE achieves competitive performance when compared to state-of-the-art NR image quality models, even without any additional fine-tuning of the CNN backbone. The learned representations are highly robust and generalize well across images afflicted by either synthetic or authentic distortions. Our results suggest that powerful quality representations with perceptual relevance can be obtained without requiring large labeled subjective image quality datasets. The implementations used in this paper are available at https://github.com/pavancm/CONTRIQUE.","tags":["Distortion;Task analysis;Image quality;Predictive models;Training;Convolutional neural networks;Computational modeling;No reference image quality assessment;blind image quality assessment;self-supervised learning;deep learning"],"title":"Image Quality Assessment Using Contrastive Learning","type":"publication"},{"authors":["Pavan C. Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"eb50e6ca2f4eac89b2232f912fa0d26c","permalink":"https://pavancm.github.io/publication/gsti-greed/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/gsti-greed/","section":"publication","summary":"We consider the problem of capturing distortions arising from changes in frame rate as part of Video Quality Assessment (VQA). Variable frame rate (VFR) videos have become much more common, and streamed videos commonly range from 30 frames per second (fps) up to 120 fps. VFR-VQA offers unique challenges in terms of distortion types as well as in making non-uniform comparisons of reference and distorted videos having different frame rates. The majority of current VQA models require compared videos to be of the same frame rate, but are unable to adequately account for frame rate artifacts. The recently proposed Generalized Entropic Difference (GREED) VQA model succeeds at this task, using natural video statistics models of entropic differences of temporal band-pass coefficients, delivering superior performance on predicting video quality changes arising from frame rate distortions. Here we propose a simple fusion framework, whereby temporal features from GREED are combined with existing VQA models, towards improving model sensitivity towards frame rate distortions. We find through extensive experiments that this feature fusion significantly boosts model performance on both HFR/VFR datasets as well as fixed frame rate (FFR) VQA databases. Our results suggest that employing efficient temporal representations can result much more robust and accurate VQA models when frame rate variations can occur.","tags":["Streaming media;Predictive models;Entropy;Band-pass filters;Video recording;Quality assessment;Rate-distortion;High frame rate;video quality assessment;full reference;entropy;natural video statistics;generalized Gaussian distribution;feature fusion"],"title":"Making Video Quality Assessment Models Sensitive to Frame Rate Distortions","type":"publication"},{"authors":["Pavan C. Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"b8549b3c393155da7da8992b5b7f654e","permalink":"https://pavancm.github.io/publication/contrique-syn/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/contrique-syn/","section":"publication","summary":"Training deep models using contrastive learning has achieved impressive performances on various computer vision tasks. Since training is done in a self-supervised manner on unlabeled data, contrastive learning is an attractive candidate for applications for which large labeled datasets are hard/expensive to obtain. In this work we investigate the outcomes of using contrastive learning on synthetically generated images for the Image Quality Assessment (IQA) problem. The training data consists of computer generated images corrupted with predetermined distortion types. Predicting distortion type and degree is used as an auxiliary task to learn image quality features. The learned representations are then used to predict quality in a No-Reference (NR) setting on real-world images. We show through extensive experiments that this model achieves comparable performance to state-of-the-art NR image quality models when evaluated on real images afflicted with synthetic distortions, even without using any real images during training. Our results indicate that training with synthetically generated images can also lead to effective, and perceptually relevant representations.","tags":["Training;Image quality;Computer vision;Computational modeling;Conferences;Training data;Distortion"],"title":"Image Quality Assessment using Synthetic Images","type":"publication"},{"authors":["Pavan C. Madhusudana","Seok-Jun Lee","Hamid R. Sheikh"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"a597db11f706f633f709c30bd69fcd1a","permalink":"https://pavancm.github.io/publication/dead-leaves/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/dead-leaves/","section":"publication","summary":"Deep neural networks targeting stereo disparity estimation have recently surpassed the performance of hand-crafted traditional models. However, training these networks require large labeled databases for obtaining accurate disparity estimates. In this letter, we address the large data requirement by generating synthetic data using natural image statistics. Images generated using dead leaves model have been shown to share many statistical characteristics commonly seen in natural images. In this work, we created a synthetic dataset using the 3D dead leaves model consisting of occluding spheres, and projected them onto parallel camera planes to obtain stereo image pairs along with ground-truth disparity map. This generated data was subsequently used to train a deep neural network in a supervised manner to estimate disparity. Through experiments we show that this trained model achieves competitive performance across real-world and synthetic stereo datasets, even without any additional fine-tuning. The proposed method for dataset generation is simplistic in nature, computationally inexpensive and can be easily scaled for large scale data generation.","tags":["Three-dimensional displays;Computational modeling;Solid modeling;Estimation;Training;Cameras;Data models;Dead leaves model;disparity estimation;disparity map;natural scene statistics;stereo matching"],"title":"Revisiting Dead Leaves Model: Training With Synthetic Data","type":"publication"},{"authors":["Pavan C. Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"85d22ce784dbd5b9a1489ee74c782adb","permalink":"https://pavancm.github.io/publication/st-greed/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/publication/st-greed/","section":"publication","summary":"We consider the problem of conducting frame rate dependent video quality assessment (VQA) on videos of diverse frame rates, including high frame rate (HFR) videos. More generally, we study how perceptual quality is affected by frame rate, and how frame rate and compression combine to affect perceived quality. We devise an objective VQA model called Space-Time GeneRalized Entropic Difference (GREED) which analyzes the statistics of spatial and temporal band-pass video coefficients. A generalized Gaussian distribution (GGD) is used to model band-pass responses, while entropy variations between reference and distorted videos under the GGD model are used to capture video quality variations arising from frame rate changes. The entropic differences are calculated across multiple temporal and spatial subbands, and merged using a learned regressor. We show through extensive experiments that GREED achieves state-of-the-art performance on the LIVE-YT-HFR Database when compared with existing VQA models. The features used in GREED are highly generalizable and obtain competitive performance even on standard, non-HFR VQA databases. The implementation of GREED has been made available online: https://github.com/pavancm/GREED.","tags":["Video recording;Quality assessment;Streaming media;Databases;Band-pass filters;Predictive models;Distortion;High frame rate;objective algorithm evaluations;video quality assessment;full reference;entropy;natural video statistics;generalized Gaussian distribution"],"title":"ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction","type":"publication"},{"authors":["Pavan C. Madhusudana","Xiangxu Yu","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"20acbd03dcc34341fa3cefb651df76fd","permalink":"https://pavancm.github.io/publication/live-yt-hfr/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/publication/live-yt-hfr/","section":"publication","summary":"High frame rate (HFR) videos are becoming increasingly common with the tremendous popularity of live, high-action streaming content such as sports. Although HFR contents are generally of very high quality, high bandwidth requirements make them challenging to deliver efficiently, while simultaneously maintaining their quality. To optimize trade-offs between bandwidth requirements and video quality, in terms of frame rate adaptation, it is imperative to understand the intricate relationship between frame rate and perceptual video quality. Towards advancing progression in this direction we designed a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR) dataset, which is comprised of 480 videos having 6 different frame rates, obtained from 16 diverse contents. In order to understand the combined effects of compression and frame rate adjustment, we also processed videos at 5 compression levels at each frame rate. To obtain subjective labels on the videos, we conducted a human study yielding 19,000 human quality ratings obtained from a pool of 85 human subjects. We also conducted a holistic evaluation of existing state-of-the-art Full and No-Reference video quality algorithms, and statistically benchmarked their performance on the new database. The LIVE-YT-HFR database has been made available online for public use and evaluation purposes, with hopes that it will help advance research in this exciting video technology direction. It may be obtained at https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html.","tags":["Videos;Databases;Quality assessment;Video recording;Distortion;Spatial resolution;Internet;High frame rate;objective algorithm evaluations;subjective quality;video quality assessment;video quality database;full reference"],"title":"Subjective and Objective Quality Assessment of High Frame Rate Videos","type":"publication"},{"authors":["Pavan C Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"89f76c7f95cc442e9476a7ee82bd39ee","permalink":"https://pavancm.github.io/publication/vmaf-greed/","publishdate":"2024-02-04T02:21:33.771599Z","relpermalink":"/publication/vmaf-greed/","section":"publication","summary":"The popularity of streaming videos with live, high-action content has led to an increased interest in High Frame Rate (HFR) videos. In this work we address the problem of frame rate dependent Video Quality Assessment (VQA) when the videos to be compared have different frame rate and compression factor. The current VQA models such as VMAF have superior correlation with perceptual judgments when videos to be compared have same frame rates and contain conventional distortions such as compression, scaling etc. However this framework requires additional pre-processing step when videos with different frame rates need to be compared, which can potentially limit its overall performance. Recently, Generalized Entropic Difference (GREED) VQA model was proposed to account for artifacts that arise due to changes in frame rate, and showed superior performance on the LIVE-YT-HFR database which contains frame rate dependent artifacts such as judder, strobing etc. In this paper we propose a simple extension, where the features from VMAF and GREED are fused in order to exploit the advantages of both models. We show through various experiments that the proposed fusion framework results in more efficient features for predicting frame rate dependent video quality. We also evaluate the fused feature set on standard non-HFR VQA databases and obtain superior performance than both GREED and VMAF, indicating the combined feature set captures complimentary perceptual quality information.","tags":["Analytical models;Correlation;Databases;Gaussian distribution;Distortion;Encoding;Quality assessment;high frame rate;video quality assessment;full reference;entropy;generalized Gaussian distribution"],"title":"High Frame Rate Video Quality Assessment using VMAF and Entropic Differences","type":"publication"},{"authors":["Pavan C. Madhusudana","Neil Birkbeck","Yilin Wang","Balu Adsumilli","Alan C. Bovik"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"d543e4de476ec7e3fdb0096a392043cb","permalink":"https://pavancm.github.io/publication/gsti/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/gsti/","section":"publication","summary":"High frame rate videos are increasingly getting popular in recent years, driven by the strong requirements of the entertainment and streaming industries to provide high quality of experiences to consumers. To achieve the best trade-offs between the bandwidth requirements and video quality in terms of frame rate adaptation, it is imperative to understand the effects of frame rate on video quality. In this direction, we devise a novel statistical entropic differencing method based on a Generalized Gaussian Distribution model expressed in the spatial and temporal band-pass domains, which measures the difference in quality between reference and distorted videos. The proposed design is highly generalizable and can be employed when the reference and distorted sequences have different frame rates. Our proposed model correlates very well with subjective scores in the recently proposed LIVE-YT-HFR database and achieves state of the art performance when compared with existing methodologies.","tags":["Entropy;Band-pass filters;Streaming media;Distortion measurement;Video recording;Quality assessment;Gaussian distribution;High frame rate;video quality assessment;full reference;entropy;natural video statistics;generalized Gaussian distribution"],"title":"Capturing Video Frame Rate Variations via Entropic Differencing","type":"publication"},{"authors":["Pavan C. Madhusudana","Rajiv Soundararajan"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"45dfb4face38ef265e30f0f9a14258ce","permalink":"https://pavancm.github.io/publication/siqe/","publishdate":"2024-02-04T02:21:33.704075Z","relpermalink":"/publication/siqe/","section":"publication","summary":"We consider the problem of quality assessment (QA) of image stitching algorithms used to generate panoramic images for virtual reality applications. Our contributions are two-fold. We design the Indian Institute of Science Stitched Image QA (ISIQA) database consisting of 264 stitched images and 6600 human quality ratings. The database consists of a variety of artifacts due to stitching such as blur, ghosting, photometric, and geometric distortions. We then devise an objective QA model called the stitched image quality evaluator (SIQE) using the statistics of steerable pyramid decompositions. In particular, we propose a Gaussian mixture model to capture the bivariate statistics of neighboring coefficients of steerable pyramid decompositions and show this to be effective in modeling the increased spatial correlation due to ghosting artifacts. We show through extensive experiments that our quality model correlates very well with subjective scores in the ISIQA database. The ISIQA database as well as the software release of SIQE has been made available online for public use and evaluation purposes.","tags":["Distortion;Databases;Image color analysis;Solid modeling;Quality assessment;Cameras;Virtual reality;Image quality assessment;virtual reality;image panorama;generalized Gaussian distribution;steerable pyramids;image stitching;Gaussian mixture model"],"title":"Subjective and Objective Quality Assessment of Stitched Images for Virtual Reality","type":"publication"},{"authors":["Navaneet K. Lakshminarasimha Murthy","Pavan C. Madhusudana","Pradyumna Suresha","Vijitha Periyasamy","Prasanta Kumar Ghosh"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"8f704c2bcc342f21f20132e7dae02706","permalink":"https://pavancm.github.io/publication/mispt/","publishdate":"2024-02-04T02:21:33.662266Z","relpermalink":"/publication/mispt/","section":"publication","summary":"We propose a multiple initialization based spectral peak tracking (MISPT) technique for heart rate monitoring from photoplethysmography (PPG) signal. MISPT is applied on the PPG signal after removing the motion artifact using an adaptive noise cancellation filter. MISPT yields several estimates of the heart rate trajectory from the spectrogram of the denoised PPG signal which are finally combined using a novel measure called trajectory strength. Multiple initializations help in correcting erroneous heart rate trajectories unlike the typical SPT which uses only single initialization. Experiments on the PPG data from 12 subjects recorded during intensive physical exercise show that the MISPT based heart rate monitoring indeed yields a better heart rate estimate compared to the SPT with single initialization. On the 12 datasets MISPT results in an average absolute error of 1.11 BPM which is lower than 1.28 BPM obtained by the state-of-the-art online heart rate monitoring algorithm.","tags":["Heart rate;Trajectory;Monitoring;Estimation;Signal processing algorithms;Photoplethysmography;Noise cancellation;Adaptive noise cancellation;heart rate monitoring;spectral peak tracking"],"title":"Multiple Spectral Peak Tracking for Heart Rate Monitoring from Photoplethysmography Signal During Intensive Physical Exercise","type":"publication"}]